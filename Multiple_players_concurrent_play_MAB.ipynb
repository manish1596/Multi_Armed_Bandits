{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_normal_dist(mu, sigma):\n",
    "    return np.random.normal(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmedBandit:\n",
    "    def __init__(self, arm_count, time_horizon, ground_truth_means):\n",
    "        self.K = arm_count\n",
    "        self.T = time_horizon\n",
    "        self.curr_t = 0\n",
    "        self.total_reward = 0\n",
    "        self.ground_truth_means = ground_truth_means\n",
    "        self.best_arm_mean = np.max(ground_truth_means)\n",
    "        self.arm_rewards = np.zeros(self.K)\n",
    "        self.reward_history = []\n",
    "        self.regret_history = []\n",
    "        self.empirical_means = np.zeros(self.K)\n",
    "        self.arm_sample_count = np.zeros(self.K, dtype = np.int32)\n",
    "        self.upper_confidence_bounds = np.zeros(self.K)\n",
    "        self.lower_confidence_bounds = np.zeros(self.K)\n",
    "    \n",
    "    def ChooseBestArm(self):\n",
    "        return np.argmax(self.upper_confidence_bounds)\n",
    "    \n",
    "    def UpdateEmpiricalMean(self, a, curr_sample):\n",
    "        self.curr_t += 1\n",
    "        self.arm_sample_count[a] += 1\n",
    "        self.arm_rewards[a] += curr_sample\n",
    "        self.empirical_means[a] = float(self.arm_rewards[a])/float(self.arm_sample_count[a])\n",
    "    \n",
    "    def UpdateConfidenceBounds(self, a):\n",
    "        self.upper_confidence_bounds[a] = self.empirical_means[a] + np.sqrt(float(2*np.log(self.curr_t))/float(self.arm_sample_count[a]))\n",
    "        self.lower_confidence_bounds[a] = self.empirical_means[a] - np.sqrt(float(2*np.log(self.curr_t))/float(self.arm_sample_count[a]))\n",
    "        \n",
    "    def Update(self, a, curr_sample):\n",
    "        self.UpdateEmpiricalMean(a, curr_sample)\n",
    "        self.UpdateConfidenceBounds(a)\n",
    "    \n",
    "    def SampleOnce(self):\n",
    "        for arm in range(self.K):\n",
    "            curr_sample = np.random.binomial(1, self.ground_truth_means[arm])\n",
    "            self.Update(arm, curr_sample)\n",
    "            self.total_reward += curr_sample\n",
    "            self.reward_history.append(self.total_reward)\n",
    "            self.regret_history.append(self.best_arm_mean*(len(self.reward_history))-self.total_reward)\n",
    "\n",
    "    def BanditPlay(self):\n",
    "        self.SampleOnce()\n",
    "        for t in range(self.T-self.K):\n",
    "            curr_arm = self.ChooseBestArm()\n",
    "            curr_sample = np.random.binomial(1, self.ground_truth_means[curr_arm])\n",
    "            self.Update(curr_arm, curr_sample)\n",
    "            self.total_reward += curr_sample\n",
    "            self.reward_history.append(self.total_reward)\n",
    "            self.regret_history.append(self.best_arm_mean*(len(self.reward_history))-self.total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplePlayers:\n",
    "    def __init__(self, a_num_players, a_margin, a_arm_count, a_time_horizon, a_arm0_ground_truth_means):\n",
    "        self.m_num_players = a_num_players\n",
    "        self.m_arm_count = a_arm_count\n",
    "        self.m_margin = a_margin\n",
    "        self.m_time_horizon = a_time_horizon\n",
    "        self.m_ground_truth_means_array = np.zeros((self.m_num_players, self.m_arm_count))\n",
    "        self.m_ground_truth_means_array[0,:] = a_arm0_ground_truth_means #np.random.uniform(size = self.m_arm_count)\n",
    "        for col in range(self.m_arm_count):\n",
    "            l = max(0.0, self.m_ground_truth_means_array[0,col]-self.m_margin)\n",
    "            h = min(self.m_ground_truth_means_array[0,col]+self.m_margin, 1.0)\n",
    "            self.m_ground_truth_means_array[1:,col] = np.random.uniform(low=l, high=h, size=self.m_num_players-1)\n",
    "        self.m_players = [MultiArmedBandit(self.m_arm_count, self.m_time_horizon, self.m_ground_truth_means_array[r,:]) for r in range(self.m_num_players)]\n",
    "\n",
    "    def UpdateConfidenceBounds(self):\n",
    "        ucb_array = np.zeros((self.m_num_players-1, self.m_arm_count))\n",
    "        for p in range(1,self.m_num_players):\n",
    "            ucb_array[p-1,:]=self.m_players[p].upper_confidence_bounds\n",
    "        min_ucbs = np.reshape(np.min(ucb_array, axis = 0), self.m_arm_count)\n",
    "        self.m_players[0].upper_confidence_bounds = np.min([self.m_players[0].upper_confidence_bounds, min_ucbs+self.m_margin], axis=0)\n",
    "    \n",
    "    def UpdateConfidenceBounds_uniform_weighted(self):\n",
    "        self.UpdateConfidenceBounds()\n",
    "        ucb_array = np.zeros((self.m_num_players-1, self.m_arm_count))\n",
    "        for a in range(self.m_arm_count):\n",
    "            for p in range(1,self.m_num_players):\n",
    "                n = self.m_players[0].arm_sample_count[a]\n",
    "                m = self.m_players[p].arm_sample_count[a]\n",
    "                X_a = self.m_players[0].arm_rewards[a]\n",
    "                Y_a = self.m_players[p].arm_rewards[a]\n",
    "                self.m_players[0].upper_confidence_bounds[a] = \\\n",
    "                    min(self.m_players[0].upper_confidence_bounds[a], \\\n",
    "                        (float(X_a+Y_a)/float(n+m))+np.sqrt(float(2*np.log(self.m_players[0].curr_t))/float(n+m))+float(m*self.m_margin)/float(n+m))\n",
    "    \n",
    "    def CalculateWeightedUCB(self, alpha, n, m, X, Y, epsilon):\n",
    "        ucb_wt = alpha*float(X)/float(n) + (1-alpha)*float(Y)/float(m) + np.sqrt(2*np.log(self.m_players[0].curr_t)*(float(alpha**2)/float(n) + float((1-alpha)**2)/float(m))) + (1-alpha)*epsilon\n",
    "        return ucb_wt\n",
    "    \n",
    "    def BestWeightedUCB(self, n, m, X, Y, epsilon):\n",
    "        #UCB_min = lambda alpha: self.CalculateWeightedUCB(alpha, n=n, m=m, X=X, Y=Y, epsilon=epsilon)\n",
    "        #print(UCB_min)\n",
    "        #res = minimize(UCB_min,(0), method='SLSQP', bounds=((0,1),), options={'disp':False, 'maxiter':100})\n",
    "        #print(res)\n",
    "#         for alpha in np.arange(0.01, 1.00, 0.01):\n",
    "#             ucb = self.CalculateWeightedUCB(alpha, n, m, X, Y, epsilon)\n",
    "#             minUCB = min(minUCB, ucb)\n",
    "        A=2*np.log(self.m_players[0].curr_t)*(float(1)/float(n)+float(1)/float(m))\n",
    "        B=-4*np.log(self.m_players[0].curr_t)/float(m)\n",
    "        C=2*np.log(self.m_players[0].curr_t)/float(m)\n",
    "        D=float(X)/float(n)-float(Y)/float(m)-epsilon\n",
    "        S=float(4*A*C*(np.power(D,2))-np.power(B*D,2))/float(4*np.power(A,3)-4*np.power(A*D,2))\n",
    "        if(S<0):\n",
    "            UCB_alpha_star = min(self.CalculateWeightedUCB(0, n, m, X, Y, epsilon), self.CalculateWeightedUCB(1, n, m, X, Y, epsilon))\n",
    "        else:\n",
    "            if(D>=0):\n",
    "                alpha_star = -np.sqrt(S) - float(B)/float(2*A)\n",
    "            else:\n",
    "                alpha_star = np.sqrt(S) - float(B)/float(2*A)\n",
    "            if(alpha_star<0):\n",
    "                alpha_star = 0\n",
    "            if(alpha_star>1):\n",
    "                alpha_star = 1\n",
    "            UCB_alpha_star = self.CalculateWeightedUCB(alpha_star, n, m, X, Y, epsilon)\n",
    "        #print(\"res.fun is \" + str(res.fun) + \" and UCB(a*) is \"+ str(UCB_alpha_star))\n",
    "        return UCB_alpha_star\n",
    "    \n",
    "    def UpdateConfidenceBounds_best_weighted(self):\n",
    "        self.UpdateConfidenceBounds()\n",
    "        for a in range(self.m_arm_count):\n",
    "            for p in range(1,self.m_num_players):\n",
    "                n = self.m_players[0].arm_sample_count[a]\n",
    "                m = self.m_players[p].arm_sample_count[a]\n",
    "                X_a = self.m_players[0].arm_rewards[a]\n",
    "                Y_a = self.m_players[p].arm_rewards[a]\n",
    "                best_ucb = self.BestWeightedUCB(n,m,X_a,Y_a,self.m_margin)\n",
    "                self.m_players[0].upper_confidence_bounds[a] = min(self.m_players[0].upper_confidence_bounds[a], best_ucb)\n",
    "\n",
    "    def ConcurrentBanditPlay(self):\n",
    "        '''\n",
    "            All the players play in parallel\n",
    "        '''\n",
    "        for player in self.m_players:\n",
    "            player.SampleOnce()\n",
    "        self.UpdateConfidenceBounds()\n",
    "        for t in range(self.m_time_horizon-self.m_arm_count):\n",
    "            for p in range(self.m_num_players):\n",
    "                p_arm = self.m_players[p].ChooseBestArm()\n",
    "                p_sample = np.random.binomial(1, self.m_players[p].ground_truth_means[p_arm])\n",
    "                self.m_players[p].Update(p_arm, p_sample)\n",
    "                self.m_players[p].total_reward += p_sample\n",
    "                self.m_players[p].reward_history.append(self.m_players[p].total_reward)\n",
    "                self.m_players[p].regret_history.append(self.m_players[p].best_arm_mean*(len(self.m_players[p].reward_history))-self.m_players[p].total_reward)\n",
    "            self.UpdateConfidenceBounds()\n",
    "            \n",
    "    def ConcurrentBanditPlay_weighted_updates(self):\n",
    "        '''\n",
    "            All the players play in parallel, alpha=n/n+m (uniform weighting)\n",
    "        '''\n",
    "        for player in self.m_players:\n",
    "            player.SampleOnce()\n",
    "        self.UpdateConfidenceBounds_uniform_weighted()\n",
    "        for t in range(self.m_time_horizon-self.m_arm_count):\n",
    "            for p in range(self.m_num_players):\n",
    "                p_arm = self.m_players[p].ChooseBestArm()\n",
    "                p_sample = np.random.binomial(1, self.m_players[p].ground_truth_means[p_arm])\n",
    "                self.m_players[p].Update(p_arm, p_sample)\n",
    "                self.m_players[p].total_reward += p_sample\n",
    "                self.m_players[p].reward_history.append(self.m_players[p].total_reward)\n",
    "                self.m_players[p].regret_history.append(self.m_players[p].best_arm_mean*(len(self.m_players[p].reward_history))-self.m_players[p].total_reward)\n",
    "            self.UpdateConfidenceBounds_uniform_weighted()\n",
    "    \n",
    "    def ConcurrentBanditPlay_best_weighting(self):\n",
    "        '''\n",
    "            All the players play in parallel, weighting is corresponding to best alpha\n",
    "        '''\n",
    "        for player in self.m_players:\n",
    "            player.SampleOnce()\n",
    "        self.UpdateConfidenceBounds_best_weighted()\n",
    "        for t in range(self.m_time_horizon-self.m_arm_count):\n",
    "            for p in range(self.m_num_players):\n",
    "                p_arm = self.m_players[p].ChooseBestArm()\n",
    "                p_sample = np.random.binomial(1, self.m_players[p].ground_truth_means[p_arm])\n",
    "                self.m_players[p].Update(p_arm, p_sample)\n",
    "                self.m_players[p].total_reward += p_sample\n",
    "                self.m_players[p].reward_history.append(self.m_players[p].total_reward)\n",
    "                self.m_players[p].regret_history.append(self.m_players[p].best_arm_mean*(len(self.m_players[p].reward_history))-self.m_players[p].total_reward)\n",
    "            self.UpdateConfidenceBounds_best_weighted()\n",
    "    \n",
    "    def ConcurrentBanditPlay_warmstart(self, warm_start_time):\n",
    "        '''\n",
    "            Other players play for a while and then base player joins in\n",
    "        '''\n",
    "        # Warm start phase\n",
    "        for player in self.m_players[1:]:\n",
    "            player.SampleOnce()\n",
    "        for player in self.m_players[1:]:\n",
    "            for t in range(warm_start_time-self.m_arm_count):\n",
    "                p_arm = player.ChooseBestArm()\n",
    "                p_sample = np.random.binomial(1, player.ground_truth_means[p_arm])\n",
    "                player.Update(p_arm, p_sample)\n",
    "                player.total_reward += p_sample\n",
    "                player.reward_history.append(player.total_reward)\n",
    "                player.regret_history.append(player.best_arm_mean*(len(player.reward_history))-player.total_reward)\n",
    "\n",
    "        # Exploration phase\n",
    "        self.m_players[0].SampleOnce()\n",
    "        self.UpdateConfidenceBounds_uniform_weighted()\n",
    "        for t in range(self.m_time_horizon-self.m_arm_count):\n",
    "            for p in range(self.m_num_players):\n",
    "                p_arm = self.m_players[p].ChooseBestArm()\n",
    "                p_sample = np.random.binomial(1, self.m_players[p].ground_truth_means[p_arm])\n",
    "                self.m_players[p].Update(p_arm, p_sample)\n",
    "                self.m_players[p].total_reward += p_sample\n",
    "                self.m_players[p].reward_history.append(self.m_players[p].total_reward)\n",
    "                self.m_players[p].regret_history.append(self.m_players[p].best_arm_mean*(len(self.m_players[p].reward_history))-self.m_players[p].total_reward)\n",
    "            self.UpdateConfidenceBounds_uniform_weighted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplePlayers_variable_epsilon:\n",
    "    def __init__(self, a_num_players, a_arm_count, a_time_horizon, a_arm0_ground_truth_means):\n",
    "        self.m_num_players = a_num_players\n",
    "        self.m_arm_count = a_arm_count\n",
    "        self.m_time_horizon = a_time_horizon\n",
    "        self.epsilon = np.zeros((self.m_num_players-1, self.m_arm_count))\n",
    "        self.m_ground_truth_means_array = np.zeros((self.m_num_players, self.m_arm_count))\n",
    "        self.m_ground_truth_means_array[0,:] = a_arm0_ground_truth_means #np.random.uniform(size = self.m_arm_count)\n",
    "        for col in range(self.m_arm_count):\n",
    "            curr_margin = abs(np.random.normal(scale=0.1))\n",
    "            l = max(0.0, self.m_ground_truth_means_array[0,col]-curr_margin)\n",
    "            h = min(self.m_ground_truth_means_array[0,col]+curr_margin, 1.0)\n",
    "            self.m_ground_truth_means_array[1:,col] = np.random.uniform(low=l, high=h, size=self.m_num_players-1)\n",
    "        self.m_players = [MultiArmedBandit(self.m_arm_count, self.m_time_horizon, self.m_ground_truth_means_array[r,:]) for r in range(self.m_num_players)]\n",
    "        print(self.m_ground_truth_means_array)\n",
    "\n",
    "    def UpdateEpsilon(self):\n",
    "        for p in range(1, self.m_num_players):\n",
    "            for a in range(self.m_arm_count):\n",
    "                self.epsilon[p-1][a] = abs(self.m_players[0].empirical_means[a]-self.m_players[p].empirical_means[a])\n",
    "                #self.epsilon[p-1][a] = max(self.m_players[p].upper_confidence_bounds[a]-self.m_players[0].lower_confidence_bounds[a], \\\n",
    "                                          #self.m_players[0].upper_confidence_bounds[a]-self.m_players[p].lower_confidence_bounds[a])\n",
    "        #print(self.epsilon)\n",
    "    \n",
    "    def UpdateConfidenceBounds(self):\n",
    "        ucb_array = np.zeros((self.m_num_players-1, self.m_arm_count))\n",
    "        for p in range(1,self.m_num_players):\n",
    "            ucb_array[p-1,:]=self.m_players[p].upper_confidence_bounds\n",
    "        min_ucbs = np.reshape(np.min(ucb_array+self.epsilon, axis = 0), self.m_arm_count)\n",
    "        self.m_players[0].upper_confidence_bounds = np.min([self.m_players[0].upper_confidence_bounds, min_ucbs], axis=0)\n",
    "\n",
    "    def CalculateWeightedUCB(self, alpha, n, m, X, Y, epsilon):\n",
    "        ucb_wt = alpha*float(X)/float(n) + (1-alpha)*float(Y)/float(m) + np.sqrt(2*np.log(self.m_players[0].curr_t)*(float(alpha**2)/float(n) + float((1-alpha)**2)/float(m))) + (1-alpha)*epsilon\n",
    "        return ucb_wt\n",
    "    \n",
    "    def BestWeightedUCB(self, n, m, X, Y, epsilon):\n",
    "        #UCB_min = lambda alpha: self.CalculateWeightedUCB(alpha, n=n, m=m, X=X, Y=Y, epsilon=epsilon)\n",
    "        #print(UCB_min)\n",
    "        #res = minimize(UCB_min,(0), method='SLSQP', bounds=((0,1),), options={'disp':False, 'maxiter':100})\n",
    "        #print(res)\n",
    "#         for alpha in np.arange(0.01, 1.00, 0.01):\n",
    "#             ucb = self.CalculateWeightedUCB(alpha, n, m, X, Y, epsilon)\n",
    "#             minUCB = min(minUCB, ucb)\n",
    "        A=2*np.log(self.m_players[0].curr_t)*(float(1)/float(n)+float(1)/float(m))\n",
    "        B=-4*np.log(self.m_players[0].curr_t)/float(m)\n",
    "        C=2*np.log(self.m_players[0].curr_t)/float(m)\n",
    "        D=float(X)/float(n)-float(Y)/float(m)-epsilon\n",
    "        S=float(4*A*C*(np.power(D,2))-np.power(B*D,2))/float(4*np.power(A,3)-4*np.power(A*D,2))\n",
    "        if(S<0):\n",
    "            UCB_alpha_star = min(self.CalculateWeightedUCB(0, n, m, X, Y, epsilon), self.CalculateWeightedUCB(1, n, m, X, Y, epsilon))\n",
    "        else:\n",
    "            if(D>=0):\n",
    "                alpha_star = -np.sqrt(S) - float(B)/float(2*A)\n",
    "            else:\n",
    "                alpha_star = np.sqrt(S) - float(B)/float(2*A)\n",
    "            if(alpha_star<0):\n",
    "                alpha_star = 0\n",
    "            if(alpha_star>1):\n",
    "                alpha_star = 1\n",
    "            UCB_alpha_star = self.CalculateWeightedUCB(alpha_star, n, m, X, Y, epsilon)\n",
    "        #print(\"res.fun is \" + str(res.fun) + \" and UCB(a*) is \"+ str(UCB_alpha_star))\n",
    "        return UCB_alpha_star\n",
    "    \n",
    "    def UpdateConfidenceBounds_best_weighted(self):\n",
    "        self.UpdateConfidenceBounds()\n",
    "        for a in range(self.m_arm_count):\n",
    "            for p in range(1,self.m_num_players):\n",
    "                n = self.m_players[0].arm_sample_count[a]\n",
    "                m = self.m_players[p].arm_sample_count[a]\n",
    "                X_a = self.m_players[0].arm_rewards[a]\n",
    "                Y_a = self.m_players[p].arm_rewards[a]\n",
    "                best_ucb = self.BestWeightedUCB(n,m,X_a,Y_a,self.epsilon[p-1][a])\n",
    "                self.m_players[0].upper_confidence_bounds[a] = min(self.m_players[0].upper_confidence_bounds[a], best_ucb)\n",
    "\n",
    "    def ConcurrentBanditPlay_best_weighting(self):\n",
    "        '''\n",
    "            All the players play in parallel and epsilon updates are based on the best estimate so far\n",
    "        '''\n",
    "        for player in self.m_players:\n",
    "            player.SampleOnce()\n",
    "        self.UpdateEpsilon()\n",
    "        self.UpdateConfidenceBounds_best_weighted()\n",
    "        for t in range(self.m_time_horizon-self.m_arm_count):\n",
    "            for p in range(self.m_num_players):\n",
    "                p_arm = self.m_players[p].ChooseBestArm()\n",
    "                p_sample = np.random.binomial(1, self.m_players[p].ground_truth_means[p_arm])\n",
    "                self.m_players[p].Update(p_arm, p_sample)\n",
    "                self.m_players[p].total_reward += p_sample\n",
    "                self.m_players[p].reward_history.append(self.m_players[p].total_reward)\n",
    "                self.m_players[p].regret_history.append(self.m_players[p].best_arm_mean*(len(self.m_players[p].reward_history))-self.m_players[p].total_reward)\n",
    "            self.UpdateEpsilon()\n",
    "            self.UpdateConfidenceBounds_best_weighted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplePlayers_epsilon_lower_bound:\n",
    "    def __init__(self, a_num_players, a_arm_count, a_base_time_horizon, a_supp_time_horizon, a_arm0_ground_truth_means):\n",
    "        self.m_num_players = a_num_players\n",
    "        self.m_arm_count = a_arm_count\n",
    "        self.m_base_time_horizon = a_base_time_horizon\n",
    "        self.m_supp_time_horizon = a_supp_time_horizon\n",
    "        self.epsilon_estimate = np.zeros((self.m_num_players-1, self.m_arm_count))+0.5\n",
    "        self.epsilon_average = np.zeros((self.m_num_players-1, self.m_arm_count))\n",
    "        self.m_ground_truth_means_array = np.zeros((self.m_num_players, self.m_arm_count))\n",
    "        \n",
    "        # Set the base player's ground truth means\n",
    "        self.m_ground_truth_means_array[0,:] = a_arm0_ground_truth_means #np.random.uniform(size = self.m_arm_count)\n",
    "        \n",
    "        # Set the supporting players' ground truth means\n",
    "        curr_margin = np.random.normal(scale=0.1, size=(self.m_num_players-1,self.m_arm_count))\n",
    "        self.m_ground_truth_means_array[1:,:] = np.clip(self.m_ground_truth_means_array[0,:]+curr_margin, 0, 1)\n",
    "        \n",
    "        self.m_players = [MultiArmedBandit(self.m_arm_count, self.m_supp_time_horizon, self.m_ground_truth_means_array[r,:]) for r in range(self.m_num_players)]\n",
    "        self.m_players[0].T = self.m_base_time_horizon\n",
    "        for p in range(1, self.m_num_players):\n",
    "            self.m_players[p].BanditPlay()\n",
    "        #print(self.m_ground_truth_means_array)\n",
    "\n",
    "    def UpdateEpsilon(self, a, p_delta=0.5):\n",
    "        for p in range(1, self.m_num_players):\n",
    "            threshold = int(2*np.log(float(1)/float(p_delta))/float(self.epsilon_estimate[p-1][a]**2))\n",
    "            if(self.m_players[0].arm_sample_count[a]>threshold):\n",
    "                if(self.epsilon_average[p-1][a]<2*self.epsilon_estimate[p-1][a]):\n",
    "                    self.epsilon_estimate[p-1][a] /= 2\n",
    "    \n",
    "    def UpdateConfidenceBounds(self):\n",
    "        ucb_array = np.zeros((self.m_num_players-1, self.m_arm_count))\n",
    "        for p in range(1,self.m_num_players):\n",
    "            ucb_array[p-1,:]=self.m_players[p].upper_confidence_bounds\n",
    "        min_ucbs = np.reshape(np.min(ucb_array+self.epsilon_estimate, axis = 0), self.m_arm_count)\n",
    "        self.m_players[0].upper_confidence_bounds = np.min([self.m_players[0].upper_confidence_bounds, min_ucbs], axis=0)\n",
    "\n",
    "    def CalculateWeightedUCB(self, alpha, n, m, X, Y, epsilon):\n",
    "        ucb_wt = alpha*float(X)/float(n) + (1-alpha)*float(Y)/float(m) + np.sqrt(2*np.log(self.m_players[0].curr_t)*(float(alpha**2)/float(n) + float((1-alpha)**2)/float(m))) + (1-alpha)*epsilon\n",
    "        return ucb_wt\n",
    "    \n",
    "    def BestWeightedUCB(self, n, m, X, Y, epsilon):\n",
    "        #UCB_min = lambda alpha: self.CalculateWeightedUCB(alpha, n=n, m=m, X=X, Y=Y, epsilon=epsilon)\n",
    "        #print(UCB_min)\n",
    "        #res = minimize(UCB_min,(0), method='SLSQP', bounds=((0,1),), options={'disp':False, 'maxiter':100})\n",
    "        #print(res)\n",
    "#         for alpha in np.arange(0.01, 1.00, 0.01):\n",
    "#             ucb = self.CalculateWeightedUCB(alpha, n, m, X, Y, epsilon)\n",
    "#             minUCB = min(minUCB, ucb)\n",
    "        A=2*np.log(self.m_players[0].curr_t)*(float(1)/float(n)+float(1)/float(m))\n",
    "        B=-4*np.log(self.m_players[0].curr_t)/float(m)\n",
    "        C=2*np.log(self.m_players[0].curr_t)/float(m)\n",
    "        D=float(X)/float(n)-float(Y)/float(m)-epsilon\n",
    "        S=float(4*A*C*(np.power(D,2))-np.power(B*D,2))/float(4*np.power(A,3)-4*np.power(A*D,2))\n",
    "        if(S<0):\n",
    "            UCB_alpha_star = min(self.CalculateWeightedUCB(0, n, m, X, Y, epsilon), self.CalculateWeightedUCB(1, n, m, X, Y, epsilon))\n",
    "        else:\n",
    "            if(D>=0):\n",
    "                alpha_star = -np.sqrt(S) - float(B)/float(2*A)\n",
    "            else:\n",
    "                alpha_star = np.sqrt(S) - float(B)/float(2*A)\n",
    "            if(alpha_star<0):\n",
    "                alpha_star = 0\n",
    "            if(alpha_star>1):\n",
    "                alpha_star = 1\n",
    "            UCB_alpha_star = self.CalculateWeightedUCB(alpha_star, n, m, X, Y, epsilon)\n",
    "        #print(\"res.fun is \" + str(res.fun) + \" and UCB(a*) is \"+ str(UCB_alpha_star))\n",
    "        return UCB_alpha_star\n",
    "    \n",
    "    def UpdateConfidenceBounds_best_weighted(self):\n",
    "        self.UpdateConfidenceBounds()\n",
    "        for a in range(self.m_arm_count):\n",
    "            for p in range(1,self.m_num_players):\n",
    "                n = self.m_players[0].arm_sample_count[a]\n",
    "                m = self.m_players[p].arm_sample_count[a]\n",
    "                X_a = self.m_players[0].arm_rewards[a]\n",
    "                Y_a = self.m_players[p].arm_rewards[a]\n",
    "                best_ucb = self.BestWeightedUCB(n,m,X_a,Y_a,self.epsilon_estimate[p-1][a])\n",
    "                self.m_players[0].upper_confidence_bounds[a] = min(self.m_players[0].upper_confidence_bounds[a], best_ucb)\n",
    "\n",
    "    def BanditPlay_best_weighting(self):\n",
    "        '''\n",
    "            The base player plays and epsilon updates are based on the best estimate so far\n",
    "        '''\n",
    "        self.m_players[0].SampleOnce()\n",
    "        for p in range(1, self.m_num_players):\n",
    "            for a in range(self.m_arm_count):\n",
    "                self.epsilon_average[p-1][a] = abs(self.m_players[0].empirical_means[a]-self.m_players[p-1].empirical_means[a])\n",
    "        for arm in range(self.m_arm_count):\n",
    "            self.UpdateEpsilon(a=arm)\n",
    "        self.UpdateConfidenceBounds_best_weighted()\n",
    "        for t in range(self.m_base_time_horizon-self.m_arm_count):\n",
    "            p_arm = self.m_players[0].ChooseBestArm()\n",
    "            #if(t<50):\n",
    "                #print(p_arm)\n",
    "            p_sample = np.random.binomial(1, self.m_players[0].ground_truth_means[p_arm])\n",
    "            self.m_players[0].Update(p_arm, p_sample)\n",
    "            self.m_players[0].total_reward += p_sample\n",
    "            self.m_players[0].reward_history.append(self.m_players[0].total_reward)\n",
    "            self.m_players[0].regret_history.append(self.m_players[0].best_arm_mean*(len(self.m_players[0].reward_history))-self.m_players[0].total_reward)\n",
    "            for p in range(1, self.m_num_players):\n",
    "                p_arm_sample_count = self.m_players[0].arm_sample_count[p_arm]\n",
    "                self.epsilon_average[p-1][p_arm] = self.epsilon_average[p-1][p_arm]*float(p_arm_sample_count-1)/float(p_arm_sample_count) + abs(p_sample-self.m_players[p].empirical_means[p_arm])/float(p_arm_sample_count)\n",
    "            self.UpdateEpsilon(a=p_arm)\n",
    "            self.UpdateConfidenceBounds_best_weighted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp 0 done\n",
      "Exp 1 done\n",
      "Exp 2 done\n",
      "Exp 3 done\n",
      "Exp 4 done\n",
      "Exp 5 done\n",
      "Exp 6 done\n",
      "Exp 7 done\n",
      "Exp 8 done\n",
      "Exp 9 done\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def experiment1(K, T, epsilon):\n",
    "    regret_histories = []\n",
    "    n_arms = K\n",
    "    time_horizon = T\n",
    "    low=0.01\n",
    "    high=0.1\n",
    "    M = MultiArmedBandit(n_arms, time_horizon, np.concatenate((np.linspace(low, high, num=n_arms-1), np.array([0.9]))))\n",
    "    M.BanditPlay()\n",
    "    regret_histories.append(M.regret_history)\n",
    "\n",
    "    max_num_players = 2\n",
    "    for p_count in range(2,max_num_players+1):\n",
    "        p_multiple = MultiplePlayers(p_count, epsilon, n_arms, time_horizon, np.concatenate((np.linspace(low, high, num=n_arms-1), np.array([0.9]))))\n",
    "        p_multiple.ConcurrentBanditPlay_best_weighting()\n",
    "        regret_histories.append(p_multiple.m_players[0].regret_history)\n",
    "    return regret_histories\n",
    "\n",
    "def experiment2(K, T, delta):\n",
    "    regret_histories = []\n",
    "    n_arms = K\n",
    "    time_horizon = T\n",
    "    low=0.5\n",
    "    high=0.5\n",
    "    M = MultiArmedBandit(n_arms, time_horizon, np.concatenate((np.array([0.5-delta]), np.linspace(low, high, num=n_arms-2), np.array([0.5+delta]))))\n",
    "    M.BanditPlay()\n",
    "    regret_histories.append(M.regret_history)\n",
    "\n",
    "    max_num_players = 2\n",
    "    for p_count in range(2,max_num_players+1):\n",
    "        p_multiple = MultiplePlayers_variable_epsilon(p_count, n_arms, time_horizon, np.concatenate((np.array([0.5-delta]) ,np.linspace(low, high, num=n_arms-2), np.array([0.5+delta]))))\n",
    "        p_multiple.ConcurrentBanditPlay_best_weighting()\n",
    "        regret_histories.append(p_multiple.m_players[0].regret_history)\n",
    "    return regret_histories\n",
    "\n",
    "def experiment3(K, T_base, T_supp, delta):\n",
    "    regret_histories = []\n",
    "    n_arms = K\n",
    "    base_time_horizon = T_base\n",
    "    supp_time_horizon = T_supp\n",
    "    low=0.5\n",
    "    high=0.5\n",
    "    M = MultiArmedBandit(n_arms, base_time_horizon, np.concatenate((np.linspace(low, high, num=n_arms-1), np.array([0.5+delta]))))\n",
    "    M.BanditPlay()\n",
    "    regret_histories.append(M.regret_history)\n",
    "\n",
    "    max_num_players = 2\n",
    "    for p_count in range(2,max_num_players+1):\n",
    "        p_multiple = MultiplePlayers_epsilon_lower_bound(p_count, n_arms, base_time_horizon, supp_time_horizon, np.concatenate((np.linspace(low, high, num=n_arms-1), np.array([0.5+delta]))))\n",
    "        p_multiple.BanditPlay_best_weighting()\n",
    "        regret_histories.append(p_multiple.m_players[0].regret_history)\n",
    "    return regret_histories\n",
    "\n",
    "exp_count = 10\n",
    "K=50\n",
    "T_base=10000\n",
    "T_supp=500\n",
    "delta=0.4\n",
    "epsilon=0.1\n",
    "avg_1 = np.zeros(T_base)\n",
    "avg_2 = np.zeros(T_base)\n",
    "avg_3 = np.zeros(T_base)\n",
    "for e in range(exp_count):\n",
    "    curr_avg_1, curr_avg_2 = experiment3(K, T_base, T_supp, delta)\n",
    "    avg_1 = (float(e)/float(e+1))*avg_1 + (1.0/float(e+1))*np.array(curr_avg_1)\n",
    "    avg_2 = (float(e)/float(e+1))*avg_2 + (1.0/float(e+1))*np.array(curr_avg_2)\n",
    "    #avg_3 = (float(e)/float(e+1))*avg_3 + (1.0/float(e+1))*np.array(curr_avg_3)\n",
    "    print(\"Exp \" + str(e) + \" done\")\n",
    "plt.plot(avg_1, color = \"b\", label = \"single player\")\n",
    "plt.plot(avg_2, color = \"r\", label = \"2 players\")\n",
    "#plt.plot(avg_3, color = \"g\", label = \"3 players\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"cumulative regret\")\n",
    "#plt.show()\n",
    "plt.savefig(\"epsilon_lower_bound_prob_delta_0p5.jpg\", dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p_multiple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-a3592fbbe908>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Variable epsilon experiments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mp_multiple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'p_multiple' is not defined"
     ]
    }
   ],
   "source": [
    "# Variable epsilon experiments\n",
    "p_multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(avg_1, color = \"b\", label = \"single player\")\n",
    "plt.plot(avg_2, color = \"r\", label = \"2 players\")\n",
    "#plt.plot(avg_3, color = \"g\", label = \"3 players\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"cumulative regret\")\n",
    "#plt.show()\n",
    "plt.savefig(\"2a_best_weighting.jpg\", dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M.ground_truth_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M.empirical_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M.arm_sample_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M.arm_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M.total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating with multiple players\n",
    "\n",
    "for p_count in range(2,11):\n",
    "    p_multiple = MultiplePlayers(p_count, 0.05, 9, 1000, np.linspace(0.1, 0.9, num=9))\n",
    "    p_multiple.ConcurrentBanditPlay()\n",
    "    plt.plot(p_multiple.m_players[0].reward_history, color = str(p_count/10), label = str(p_count))\n",
    "\n",
    "plt.show()\n",
    "players_2 = MultiplePlayers(10, 0.05, 9, 1000, np.linspace(0.1, 0.9, num=9))\n",
    "#print(players_2.m_ground_truth_means_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_2.ConcurrentBanditPlay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(players_2.m_ground_truth_means_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(players_2.m_players[0].ground_truth_means)\n",
    "print(players_2.m_players[0].empirical_means)\n",
    "print(players_2.m_players[0].arm_rewards)\n",
    "print(players_2.m_players[0].arm_sample_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(players_2.m_players[5].ground_truth_means)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(0.1, 0.1, num=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(players_2.m_players[0].total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
