{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_normal_dist(mu, sigma):\n",
    "    return np.random.normal(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmedBandit:\n",
    "    def __init__(self, arm_count, time_horizon, ground_truth_means):\n",
    "        self.K = arm_count\n",
    "        self.T = time_horizon\n",
    "        self.curr_t = 0\n",
    "        self.total_reward = 0\n",
    "        self.ground_truth_means = ground_truth_means\n",
    "        self.best_arm_mean = np.max(ground_truth_means)\n",
    "        self.arm_rewards = np.zeros(self.K)\n",
    "        self.reward_history = []\n",
    "        self.regret_history = []\n",
    "        self.empirical_means = np.zeros(self.K)\n",
    "        self.arm_sample_count = np.zeros(self.K, dtype = np.int32)\n",
    "        self.upper_confidence_bounds = np.zeros(self.K)\n",
    "        self.lower_confidence_bounds = np.zeros(self.K)\n",
    "    \n",
    "    def ChooseBestArm(self):\n",
    "        return np.argmax(self.upper_confidence_bounds)\n",
    "    \n",
    "    def UpdateEmpiricalMean(self, a, curr_sample):\n",
    "        self.curr_t += 1\n",
    "        self.arm_sample_count[a] += 1\n",
    "        self.arm_rewards[a] += curr_sample\n",
    "        self.empirical_means[a] = float(self.arm_rewards[a])/float(self.arm_sample_count[a])\n",
    "    \n",
    "    def UpdateConfidenceBounds(self, a):\n",
    "        T=self.T\n",
    "        self.upper_confidence_bounds[a] = self.empirical_means[a] + np.sqrt(float(2*np.log(T))/float(self.arm_sample_count[a]))\n",
    "        self.lower_confidence_bounds[a] = self.empirical_means[a] - np.sqrt(float(2*np.log(T))/float(self.arm_sample_count[a]))\n",
    "        \n",
    "    def Update(self, a, curr_sample):\n",
    "        self.UpdateEmpiricalMean(a, curr_sample)\n",
    "        self.UpdateConfidenceBounds(a)\n",
    "    \n",
    "    def SampleOnce(self):\n",
    "        for arm in range(self.K):\n",
    "            curr_sample = np.random.binomial(1, self.ground_truth_means[arm])\n",
    "            self.Update(arm, curr_sample)\n",
    "            self.total_reward += curr_sample\n",
    "            self.reward_history.append(self.total_reward)\n",
    "            self.regret_history.append(self.best_arm_mean*(len(self.reward_history))-self.total_reward)\n",
    "\n",
    "    def BanditPlay(self):\n",
    "        self.SampleOnce()\n",
    "        for t in range(self.T-self.K):\n",
    "            curr_arm = self.ChooseBestArm()\n",
    "            curr_sample = np.random.binomial(1, self.ground_truth_means[curr_arm])\n",
    "            self.Update(curr_arm, curr_sample)\n",
    "            self.total_reward += curr_sample\n",
    "            self.reward_history.append(self.total_reward)\n",
    "            self.regret_history.append(self.best_arm_mean*(len(self.reward_history))-self.total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplePlayers:\n",
    "    def __init__(self, a_num_players, a_margin, a_arm_count, a_time_horizon, seed_ground_truth_means):\n",
    "        self.m_num_players = a_num_players\n",
    "        self.m_arm_count = a_arm_count\n",
    "        self.m_margin = a_margin\n",
    "        self.m_time_horizon = a_time_horizon\n",
    "        self.m_ground_truth_means_array = np.zeros((self.m_num_players, self.m_arm_count))\n",
    "        for col in range(self.m_arm_count):\n",
    "            l = max(0.0, seed_ground_truth_means[col]-self.m_margin)\n",
    "            h = min(seed_ground_truth_means[col]+self.m_margin, 1.0)\n",
    "            self.m_ground_truth_means_array[:,col] = np.random.uniform(low=l, high=h, size=self.m_num_players)\n",
    "        self.m_players = [MultiArmedBandit(self.m_arm_count, self.m_time_horizon, self.m_ground_truth_means_array[r,:]) for r in range(self.m_num_players)]\n",
    "\n",
    "    def CalculateUCB_width(self, alpha, n, m, X, Y, epsilon):\n",
    "        T=self.m_time_horizon\n",
    "        ucb_wt = np.sqrt(2*np.log(T)*(float(alpha**2)/float(n) + float((1-alpha)**2)/float(m))) + (1-alpha)*epsilon\n",
    "        return ucb_wt\n",
    "\n",
    "    def BestWeightedUCB(self, n, m, X, Y, epsilon):\n",
    "        T=self.m_time_horizon\n",
    "        A=2*np.log(T)*(float(1)/float(n)+float(1)/float(m))\n",
    "        B=-4*np.log(T)/float(m)\n",
    "        C=2*np.log(T)/float(m)\n",
    "        D=-1*epsilon\n",
    "        S=float(4*A*C*(np.power(D,2))-np.power(B*D,2))/float(4*np.power(A,3)-4*np.power(A*D,2))\n",
    "        if(S<0):\n",
    "            if(self.CalculateUCB_width(0, n, m, X, Y, epsilon) < self.CalculateUCB_width(1, n, m, X, Y, epsilon)):\n",
    "                UCB_alpha_star =  float(Y)/float(m) + self.CalculateUCB_width(0, n, m, X, Y, epsilon)\n",
    "            else:\n",
    "                UCB_alpha_star =  float(X)/float(n) + self.CalculateUCB_width(1, n, m, X, Y, epsilon)\n",
    "        else:\n",
    "            if(D>=0):\n",
    "                alpha_star = -np.sqrt(S) - float(B)/float(2*A)\n",
    "            else:\n",
    "                alpha_star = np.sqrt(S) - float(B)/float(2*A)\n",
    "            if(alpha_star<0):\n",
    "                alpha_star = 0\n",
    "            if(alpha_star>1):\n",
    "                alpha_star = 1\n",
    "            UCB_alpha_star = alpha_star*float(X)/float(n) + (1-alpha_star)*float(Y)/float(m) + self.CalculateUCB_width(alpha_star, n, m, X, Y, epsilon)\n",
    "        return UCB_alpha_star\n",
    "\n",
    "    def UpdateConfidenceBounds_best_weighted(self, curr_player):\n",
    "        for a in range(self.m_arm_count):\n",
    "            m_other_players = 0\n",
    "            Y_a = 0\n",
    "            for p in range(self.m_num_players):\n",
    "                if(p != curr_player):\n",
    "                    m_other_players += self.m_players[p].arm_sample_count[a]\n",
    "                    Y_a += self.m_players[p].arm_rewards[a]\n",
    "            n = self.m_players[curr_player].arm_sample_count[a]\n",
    "            X_a = self.m_players[curr_player].arm_rewards[a]\n",
    "            best_ucb = self.BestWeightedUCB(n,m,X_a,Y_a,self.m_margin)\n",
    "            self.m_players[curr_player].upper_confidence_bounds[a] = best_ucb\n",
    "\n",
    "    def ConcurrentBanditPlay_best_weighting(self):\n",
    "        '''\n",
    "            All the players play in parallel, weighting is corresponding to best alpha\n",
    "        '''\n",
    "        for player in self.m_players:\n",
    "            player.SampleOnce()\n",
    "        for t in range(self.m_time_horizon-self.m_arm_count):\n",
    "            for p in range(self.m_num_players):\n",
    "                p_arm = self.m_players[p].ChooseBestArm()\n",
    "                p_sample = np.random.binomial(1, self.m_players[p].ground_truth_means[p_arm])\n",
    "                self.m_players[p].Update(p_arm, p_sample)\n",
    "                self.m_players[p].total_reward += p_sample\n",
    "                self.m_players[p].reward_history.append(self.m_players[p].total_reward)\n",
    "                self.m_players[p].regret_history.append(self.m_players[p].best_arm_mean*(len(self.m_players[p].reward_history))-self.m_players[p].total_reward)\n",
    "            for p in range(self.m_num_players):\n",
    "                self.UpdateConfidenceBounds_best_weighted(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
