{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_normal_dist(mu, sigma):\n",
    "    return np.random.normal(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmedBandit:\n",
    "    def __init__(self, arm_count, time_horizon, ground_truth_means):\n",
    "        self.K = arm_count\n",
    "        self.T = time_horizon\n",
    "        self.curr_t = 0\n",
    "        self.total_reward = 0\n",
    "        self.ground_truth_means = ground_truth_means\n",
    "        self.best_arm_mean = np.max(ground_truth_means)\n",
    "        self.arm_rewards = np.zeros(self.K)\n",
    "        self.reward_history = []\n",
    "        self.regret_history = []\n",
    "        self.empirical_means = np.zeros(self.K)\n",
    "        self.arm_sample_count = np.zeros(self.K, dtype = np.int32)\n",
    "        self.upper_confidence_bounds = np.zeros(self.K)\n",
    "        self.lower_confidence_bounds = np.zeros(self.K)\n",
    "    \n",
    "    def ChooseBestArm(self):\n",
    "        return np.argmax(self.upper_confidence_bounds)\n",
    "    \n",
    "    def UpdateEmpiricalMean(self, a, curr_sample):\n",
    "        self.curr_t += 1\n",
    "        self.arm_sample_count[a] += 1\n",
    "        self.arm_rewards[a] += curr_sample\n",
    "        self.empirical_means[a] = float(self.arm_rewards[a])/float(self.arm_sample_count[a])\n",
    "    \n",
    "    def UpdateConfidenceBounds(self, a):\n",
    "        T=self.T\n",
    "        self.upper_confidence_bounds[a] = self.empirical_means[a] + np.sqrt(float(2*np.log(T))/float(self.arm_sample_count[a]))\n",
    "        self.lower_confidence_bounds[a] = self.empirical_means[a] - np.sqrt(float(2*np.log(T))/float(self.arm_sample_count[a]))\n",
    "        \n",
    "    def Update(self, a, curr_sample):\n",
    "        self.UpdateEmpiricalMean(a, curr_sample)\n",
    "        self.UpdateConfidenceBounds(a)\n",
    "    \n",
    "    def SampleOnce(self):\n",
    "        for arm in range(self.K):\n",
    "            curr_sample = np.random.binomial(1, self.ground_truth_means[arm])\n",
    "            self.Update(arm, curr_sample)\n",
    "            self.total_reward += curr_sample\n",
    "            self.reward_history.append(self.total_reward)\n",
    "            self.regret_history.append(self.best_arm_mean*(len(self.reward_history))-self.total_reward)\n",
    "\n",
    "    def BanditPlay(self):\n",
    "        self.SampleOnce()\n",
    "        for t in range(self.T-self.K):\n",
    "            curr_arm = self.ChooseBestArm()\n",
    "            curr_sample = np.random.binomial(1, self.ground_truth_means[curr_arm])\n",
    "            self.Update(curr_arm, curr_sample)\n",
    "            self.total_reward += curr_sample\n",
    "            self.reward_history.append(self.total_reward)\n",
    "            self.regret_history.append(self.best_arm_mean*(len(self.reward_history))-self.total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplePlayers:\n",
    "    def __init__(self, a_num_players, a_margin, a_arm_count, a_time_horizon, assumption2):\n",
    "        self.m_num_players = a_num_players\n",
    "        self.m_arm_count = a_arm_count\n",
    "        self.m_margin = a_margin\n",
    "        self.m_time_horizon = a_time_horizon\n",
    "        self.m_ground_truth_means_array = np.zeros((self.m_num_players, self.m_arm_count))\n",
    "        if(assumption2==false):\n",
    "            seed_ground_truth_means = np.uniform(low=0.0, high=1.0, size=self.m_arm_count)\n",
    "            for col in range(self.m_arm_count):\n",
    "                l = max(0.0, seed_ground_truth_means[col]-0.5*self.m_margin)\n",
    "                h = min(seed_ground_truth_means[col]+0.5*self.m_margin, 1.0)\n",
    "                self.m_ground_truth_means_array[:,col] = np.random.uniform(low=l, high=h, size=self.m_num_players)\n",
    "            self.m_players = [MultiArmedBandit(self.m_arm_count, self.m_time_horizon, self.m_ground_truth_means_array[r,:]) for r in range(self.m_num_players)]\n",
    "        else:\n",
    "            self.m_ground_truth_means_array[0,-2] = np.random.uniform(low = 0.0, high=1.0-3*self.m_margin)\n",
    "            self.m_ground_truth_means_array[0,-1] = np.random.uniform(low = self.m_ground_truth_means_array[0,-2]+3*self.m_margin, high=1.0)\n",
    "            self.m_ground_truth_means_array[0,:-2] = np.random.uniform(low = 0.0, high=self.m_ground_truth_means_array[0,-2], size=(self.m_arm_count-2))\n",
    "            for col in range(self.m_arm_count):\n",
    "                l = max(0.0, self.m_ground_truth_means_array[0,col]-0.5*self.m_margin)\n",
    "                h = min(self.m_ground_truth_means_array[0,col]+0.5*self.m_margin, 1.0)\n",
    "                self.m_ground_truth_means_array[1:,col] = np.random.uniform(low=l, high=h, size=self.m_num_players-1)\n",
    "            self.m_players = [MultiArmedBandit(self.m_arm_count, self.m_time_horizon, self.m_ground_truth_means_array[r,:]) for r in range(self.m_num_players)]\n",
    "\n",
    "    def CalculateUCB_width(self, alpha, n, m, X, Y, epsilon):\n",
    "        T=self.m_time_horizon\n",
    "        ucb_width = np.sqrt(2*np.log(T)*(float(alpha**2)/float(n) + float((1-alpha)**2)/float(m))) + (1-alpha)*epsilon\n",
    "        return ucb_width\n",
    "\n",
    "    def BestWeightedUCB(self, n, m, X, Y, epsilon):\n",
    "        T=self.m_time_horizon\n",
    "        A=2*np.log(T)*(float(1)/float(n)+float(1)/float(m))\n",
    "        B=-4*np.log(T)/float(m)\n",
    "        C=2*np.log(T)/float(m)\n",
    "        D=-1*epsilon\n",
    "        S=float(4*A*C*(np.power(D,2))-np.power(B*D,2))/float(4*np.power(A,3)-4*np.power(A*D,2))\n",
    "        if(S<0):\n",
    "            if(self.CalculateUCB_width(0, n, m, X, Y, epsilon) < self.CalculateUCB_width(1, n, m, X, Y, epsilon)):\n",
    "                UCB_alpha_star =  float(Y)/float(m) + self.CalculateUCB_width(0, n, m, X, Y, epsilon)\n",
    "            else:\n",
    "                UCB_alpha_star =  float(X)/float(n) + self.CalculateUCB_width(1, n, m, X, Y, epsilon)\n",
    "        else:\n",
    "            if(D>=0):\n",
    "                alpha_star = -np.sqrt(S) - float(B)/float(2*A)\n",
    "            else:\n",
    "                alpha_star = np.sqrt(S) - float(B)/float(2*A)\n",
    "            if(alpha_star<0):\n",
    "                alpha_star = 0\n",
    "            if(alpha_star>1):\n",
    "                alpha_star = 1\n",
    "            ucb_width = self.CalculateUCB_width(alpha_star, n, m, X, Y, epsilon)\n",
    "            UCB_alpha_star = alpha_star*float(X)/float(n) + (1-alpha_star)*float(Y)/float(m) + ucb_width\n",
    "        return UCB_alpha_star\n",
    "\n",
    "    def UpdateConfidenceBounds(self, curr_player, weighting):\n",
    "        for a in range(self.m_arm_count):\n",
    "            m_other_players = 0\n",
    "            Y_a = 0\n",
    "            for p in range(self.m_num_players):\n",
    "                if(p != curr_player):\n",
    "                    m_other_players += self.m_players[p].arm_sample_count[a]\n",
    "                    Y_a += self.m_players[p].arm_rewards[a]\n",
    "            n = self.m_players[curr_player].arm_sample_count[a]\n",
    "            X_a = self.m_players[curr_player].arm_rewards[a]\n",
    "            if(weighting == \"best\"):\n",
    "                ucb = self.BestWeightedUCB(n, m_other_players, X_a, Y_a, self.m_margin)\n",
    "            elif(weighting == \"uniform\"):\n",
    "                alpha = float(n)/float(n + m_other_players)\n",
    "                ucb_width = self.CalculateUCB_width(alpha, n, m_other_players, X_a, Y_a, self.m_margin)\n",
    "                ucb = alpha*float(X_a)/float(n) + (1-alpha)*float(Y_a)/float(m_other_players) + ucb_width\n",
    "            elif(weighting == \"zero\"):\n",
    "                ucb_width = self.CalculateUCB_width(0, n, m_other_players, X_a, Y_a, self.m_margin)\n",
    "                ucb = float(Y_a)/float(m_other_players) + ucb_width\n",
    "            elif(weighting == \"one\"):\n",
    "                ucb_width = self.CalculateUCB_width(1, n, m_other_players, X_a, Y_a, self.m_margin)\n",
    "                ucb = float(X_a)/float(n) + ucb_width\n",
    "            self.m_players[curr_player].upper_confidence_bounds[a] = ucb\n",
    "\n",
    "    def ConcurrentBanditPlay(self, weighting):\n",
    "        '''\n",
    "            All the players play in parallel, weighting is either \"best\", \"uniform\", \"zero\" or \"one\"\n",
    "        '''\n",
    "        for player in self.m_players:\n",
    "            player.SampleOnce()\n",
    "        for t in range(self.m_time_horizon-self.m_arm_count):\n",
    "            for p in range(self.m_num_players):\n",
    "                p_arm = self.m_players[p].ChooseBestArm()\n",
    "                p_sample = np.random.binomial(1, self.m_players[p].ground_truth_means[p_arm])\n",
    "                self.m_players[p].Update(p_arm, p_sample)\n",
    "                self.m_players[p].total_reward += p_sample\n",
    "                self.m_players[p].reward_history.append(self.m_players[p].total_reward)\n",
    "                self.m_players[p].regret_history.append(self.m_players[p].best_arm_mean*(len(self.m_players[p].reward_history))-self.m_players[p].total_reward)\n",
    "            for p in range(self.m_num_players):\n",
    "                self.UpdateConfidenceBounds(p, weighting)\n",
    "                \n",
    "    def WarmstartBanditPlay(self, num_warmstart_players, weighting):\n",
    "        '''\n",
    "            A given number of players play independently initially and the remaining players warmstart off them\n",
    "        '''\n",
    "        for p in range(num_warmstart_players):\n",
    "            self.m_players[p].BanditPlay()\n",
    "\n",
    "        for player in self.m_players[num_warmstart_players:]:\n",
    "            player.SampleOnce()\n",
    "        for p in range(self.m_num_players):\n",
    "            self.UpdateConfidenceBounds(p, weighting)\n",
    "        for t in range(self.m_time_horizon-self.m_arm_count):\n",
    "            for p in range(num_warmstart_players, self.m_num_players):\n",
    "                p_arm = self.m_players[p].ChooseBestArm()\n",
    "                p_sample = np.random.binomial(1, self.m_players[p].ground_truth_means[p_arm])\n",
    "                self.m_players[p].Update(p_arm, p_sample)\n",
    "                self.m_players[p].total_reward += p_sample\n",
    "                self.m_players[p].reward_history.append(self.m_players[p].total_reward)\n",
    "                self.m_players[p].regret_history.append(self.m_players[p].best_arm_mean*(len(self.m_players[p].reward_history))-self.m_players[p].total_reward)\n",
    "            for p in range(self.m_num_players):\n",
    "                self.UpdateConfidenceBounds(p, weighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment1():\n",
    "    '''\n",
    "    Comparing results with different number of players: M = 2, 10, 100 and under best weighting\n",
    "    '''\n",
    "    M_list = [2, 10, 100]\n",
    "    K=10\n",
    "    T=1e4\n",
    "    epsilon = 0.1\n",
    "    regret_list = []\n",
    "    for M in M_list:\n",
    "        players = MultiplePlayers(M, epsilon, K, T, assumption2=true)\n",
    "        players.ConcurrentBanditPlay(weighting=\"best\")\n",
    "        #regret_list.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment2():\n",
    "    '''\n",
    "    Comparing results with different relative values of epsilon and delta\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M2_avg_regret = np.zeros(T)\n",
    "M10_avg_regret = np.zeros(T)\n",
    "M100_avg_regret = np.zeros(T)\n",
    "experiment_count = 50\n",
    "for e in range(experiment_count):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
